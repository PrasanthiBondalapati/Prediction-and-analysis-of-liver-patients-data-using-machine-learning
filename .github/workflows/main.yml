# -*- coding: utf-8 -*-
"""Copy of final Liver_Disease_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pFeKOpF7bcWtWZY_NgF5r9_xavOOgEYE

**LOADING THE DATA**
"""



! unzip '/content/archive.zip'

"""**IMPORTING LIBRARIES**"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pickle

"""**READING THE DATASET**"""

df = pd.read_csv("/content/indian_liver_patient.csv")

"""**EXPLORATORY DATA ANALYSIS**"""

df.head(5)

df.tail()

df = df.drop_duplicates()

df.shape

df.value_counts('Dataset')

df.describe()

df.info()

"""**CHECKING NULL VALUES**"""

df.isnull().any()

"""**HANDLING NULL VALUES**"""

mode_value = df['Albumin_and_Globulin_Ratio'].mode()[0]
df['Albumin_and_Globulin_Ratio'] = df['Albumin_and_Globulin_Ratio'].fillna(mode_value)

df.isnull().sum()

"""**DATA VISUALIZATION**

UNIVARIATE
"""

sns.countplot(data= df, x = 'Gender',label = 'Count')
m,f = df['Gender'].value_counts()
print("No of Males: ",m)
print("No of Females: ",f)

sns.countplot(data= df, x= 'Dataset')
LD,NLD = df['Dataset'].value_counts()
print("liver disease patients: ",LD)
print("Non-liver disease patients: ",NLD)

plt.pie(df['Dataset'].value_counts(),[0,0.1],labels=['LIVER_DISEASE','NON-LIVER_DISEASE'],autopct ="%1.1f%%",shadow = True,colors = ['#73C5C5','#BDE2B9'])
plt.title("LIVER DISEASE")
plt.show()

"""BIVARIATE"""

sns.scatterplot(x = df['Albumin_and_Globulin_Ratio'], y = df['Dataset'],palette = 'Set2',data = df)

"""MULTI-VARIATE"""

sns.pairplot(df,hue = 'Gender',diag_kind = 'kde')

"""CORRELATION"""

sns.heatmap(df.corr())

df.corr()['Dataset'].sort_values(ascending = False)

df['Gender']=df['Gender'].map({'Male':1,'Female':0})
df['Gender']=df['Gender'].astype('int64')

"""**SPLITTING THE DATASET**"""

x = df.iloc[0:400,0:-1]
y = df.iloc[0:400,-1]
x.head()

y.head()

y.value_counts()

"""**BALANCING THE DATASET**"""

from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTETomek
smote = SMOTETomek(sampling_strategy='auto')
x_bal,y_bal = smote.fit_resample(x,y)
print(y.value_counts())
print(y_bal.value_counts())

"""**SCALING THE DATA**"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
xtrain_scaled = scaler.fit_transform(xtrain)
xtest_scaled = scaler.transform(xtest)
names=x.columns
x_bal=pd.DataFrame(x_bal,columns=names)
x_bal.head()

x_bal.tail()

y_bal.head()

"""**TRAINING AND TESTING**"""

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain, ytest = train_test_split(x_bal,y_bal,test_size=0.2,random_state=49)

x_bal.tail()

y_bal.head()

xtrain.shape

xtest.shape

"""**MODEL BUILDING**"""

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

"""SUPPORT VECTOR MACHINE MODEL"""

svm = SVC()
RFmodel=RandomForestClassifier()
KNNmodel= KNeighborsClassifier()
scaler = StandardScaler()
xtrain = scaler.fit_transform(xtrain)
xtest = scaler.transform(xtest)
svm.fit(xtrain,ytrain)

SVMPred = svm.predict(xtest)
SVMaccuracy = accuracy_score(ytest, SVMPred)
SVMaccuracy

def svm_model(X_train, X_test, y_train, y_test):
    svm = SVC()
    svm.fit(X_train, y_train)
    SVMpred = svm.predict(X_test)
    SVMaccuracy = accuracy_score(y_test, SVMpred)
    print("SVM Accuracy Score: {}".format(SVMaccuracy))
    print("Classification Report:\n", classification_report(y_test, SVMpred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, SVMpred))
svm_model(xtrain, xtest, ytrain, ytest)

"""KNN MODEL"""

KNNmodel.fit(xtrain,ytrain)

KNNpred = KNNmodel.predict(xtest)
KNNaccuracy = accuracy_score(KNNpred,ytest)
KNNaccuracy

def knn(X_train, X_test, y_train, y_test):
    knn_model = KNeighborsClassifier()
    knn_model.fit(X_train, y_train)
    KNNpred = knn_model.predict(X_test)
    KNNaccuracy = accuracy_score(y_test, KNNpred)
    print("KNN Accuracy Score: {}".format(KNNaccuracy))
    print("Classification Report:\n", classification_report(y_test, KNNpred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, KNNpred))
knn(xtrain, xtest, ytrain, ytest)

"""RANDOM FOREST MODEL"""

RFmodel.fit(xtrain,ytrain)

RFpred=RFmodel.predict(xtest)
RFaccuracy = accuracy_score(RFpred,ytest)
RFaccuracy

def random_forest(xtrain, xtest, ytrain, ytest):
    rf = RandomForestClassifier()
    rf.fit(xtrain, ytrain)
    RFpred = rf.predict(xtest)
    RFaccuracy = accuracy_score(ytest, RFpred)
    print("Random Forest Accuracy Score: {}".format(RFaccuracy))
    print("Classification Report:\n", classification_report(ytest, RFpred))
    print("Confusion Matrix:\n", confusion_matrix(ytest, RFpred))
random_forest(xtrain, xtest, ytrain, ytest)

"""DECISION TREE CLASSIFIER"""

from sklearn.tree import DecisionTreeClassifier
dt_model = DecisionTreeClassifier()
dt_model.fit(xtrain, ytrain)

dt_pred = dt_model.predict(xtest)
DTaccuracy = accuracy_score(ytest, dt_pred)
DTaccuracy

def decision_tree(X_train, X_test, y_train, y_test):
    dt = DecisionTreeClassifier()
    dt.fit(X_train, y_train)
    y_pred = dt.predict(X_test)
    print("Decision Tree Accuracy Score: {}".format(accuracy_score(y_test, y_pred)))
    print("Classification Report:\n", classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
decision_tree(xtrain, xtest, ytrain, ytest)

"""LOGISTIC REGRESSION MODEL"""

from sklearn.linear_model import LogisticRegression
log_reg_model = LogisticRegression()
log_reg_model.fit(xtrain, ytrain)

y_pred = log_reg_model.predict(xtest)
log_reg_accuracy = accuracy_score(ytest, y_pred)
log_reg_accuracy

def logistic_regression(X_train, X_test, y_train, y_test):
    lr = LogisticRegression(max_iter=1000)
    lr.fit(X_train, y_train)
    LRpred = lr.predict(X_test)
    LRaccuracy = accuracy_score(y_test, LRpred)
    print("Logistic Regression Accuracy Score: {}".format(LRaccuracy))
    print("Classification Report:\n", classification_report(y_test, LRpred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, LRpred))
logistic_regression(xtrain, xtest, ytrain, ytest)

"""COMPARING THE MODELS"""

svm = SVC()
dt_model = DecisionTreeClassifier()
RFmodel = RandomForestClassifier()
KNNmodel = KNeighborsClassifier()
log_reg_model = LogisticRegression()
svm.fit(xtrain,ytrain)
dt_model.fit(xtrain,ytrain)
RFmodel.fit(xtrain,ytrain)
KNNmodel.fit(xtrain,ytrain)
log_reg_model.fit(xtrain,ytrain)
pred0=svm.predict(xtrain)
pred1=dt_model.predict(xtrain)
pred2=RFmodel.predict(xtrain)
pred3=KNNmodel.predict(xtrain)
pred4=log_reg_model.predict(xtrain)

print('SVM:',accuracy_score(ytrain,pred0))
print('Decision Tree:',accuracy_score(ytrain,pred1))
print('Random Forest:',accuracy_score(ytrain,pred2))
print('KNN:',accuracy_score(ytrain,pred3))
print('Logistic:',accuracy_score(ytrain,pred4))

y_pred1=svm.predict(xtest)
y_pred2=dt_model.predict(xtest)
y_pred3=RFmodel.predict(xtest)
y_pred4=KNNmodel.predict(xtest)
y_pred5 =log_reg_model.predict(xtest)

print('Support Vector:',accuracy_score(ytest,y_pred1))
print('Decision Tree:',accuracy_score(ytest,y_pred2))
print('Random Forest:',accuracy_score(ytest,y_pred3))
print('KNN:',accuracy_score(ytest,y_pred4))
print('Logistic:',accuracy_score(ytest,y_pred5))

"""**HYPERPARAMETER TUNNING**"""

def randomForest(xtrain,xtest,ytrain,ytest):
  model = RandomForestClassifier(verbose=2,n_estimators=120,max_features='log2',max_depth=10,criterion='entropy')
  model.fit(xtrain,ytrain)
  y_tr=model.predict(xtrain)
  print("Accuracy Score {}".format(accuracy_score(ytrain,y_tr)))
  y_pr=model.predict(xtest)
  print("Accuracy Score {}".format(accuracy_score(ytest,y_pr)))
randomForest(xtrain,xtest,ytrain,ytest)

from sklearn.metrics import f1_score
RFmodel.fit(xtrain,ytrain)
y_pred3 = RFmodel.predict(xtest)
print("Accuracy Score {}".format(accuracy_score(ytest,y_pred3)))
print("f1_Score {}".format(f1_score(y_pred3,ytest,average='weighted')))

from sklearn.model_selection import GridSearchCV

# Hyperparameter tuning for SVM
svm_params = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

svm_grid = GridSearchCV(SVC(), svm_params, cv=5)
svm_grid.fit(xtrain, ytrain)

# Best model
best_svm = svm_grid.best_estimator_
y_pred_svm = best_svm.predict(xtest)

# Evaluation
print("SVM Accuracy: ", accuracy_score(ytest, y_pred_svm))
print("Classification Report:\n", classification_report(ytest, y_pred_svm))
print("Confusion Matrix:\n", confusion_matrix(ytest, y_pred_svm))

print(best_svm)

# Hyperparameter tuning for Decision Tree
dt_params = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

dt_grid = GridSearchCV(DecisionTreeClassifier(), dt_params, cv=5)
dt_grid.fit(xtrain, ytrain)

# Best model
best_dt = dt_grid.best_estimator_
y_pred_dt = best_dt.predict(xtest)

# Evaluation
print("Decision Tree Accuracy: ", accuracy_score(ytest, y_pred_dt))
print("Classification Report:\n", classification_report(ytest, y_pred_dt))
print("Confusion Matrix:\n", confusion_matrix(ytest, y_pred_dt))

print(best_dt)

# Hyperparameter tuning for Random Forest
rf_params = {
    'n_estimators': [10, 50, 100],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

rf_grid = GridSearchCV(RandomForestClassifier(), rf_params, cv=5)
rf_grid.fit(xtrain, ytrain)

# Best model
best_rf = rf_grid.best_estimator_
y_pred_rf = best_rf.predict(xtest)

# Evaluation
print("Random Forest Accuracy: ", accuracy_score(ytest, y_pred_rf))
print("Classification Report:\n", classification_report(ytest, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(ytest, y_pred_rf))

print(best_rf)

# Hyperparameter tuning for KNN
knn_params = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance']
}

knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)
knn_grid.fit(xtrain, ytrain)

# Best model
best_knn = knn_grid.best_estimator_
y_pred_knn = best_knn.predict(xtest)

# Evaluation
print("KNN Accuracy: ", accuracy_score(ytest, y_pred_knn))
print("Classification Report:\n", classification_report(ytest, y_pred_knn))
print("Confusion Matrix:\n", confusion_matrix(ytest, y_pred_knn))

print(best_knn)

# Hyperparameter tuning for Logistic Regression
lr_params = {
    'C': [0.001, 0.01, 0.1, 1, 10],
    'solver': ['liblinear', 'saga']
}

lr_grid = GridSearchCV(LogisticRegression(), lr_params, cv=5)
lr_grid.fit(xtrain, ytrain)

# Best model
best_lr = lr_grid.best_estimator_
y_pred_lr = best_lr.predict(xtest)

# Evaluation
print("Logistic Regression Accuracy: ", accuracy_score(ytest, y_pred_lr))
print("Classification Report:\n", classification_report(ytest, y_pred_lr))
print("Confusion Matrix:\n", confusion_matrix(ytest, y_pred_lr))

print(best_lr)

"""**EVALUATING PERFORMANCE OF THE MODEL & SAVING THE MODEL**"""

from sklearn.metrics import f1_score
RFmodel.fit(xtrain,ytrain)
y_pred3 = RFmodel.predict(xtest)
print("Accuracy Score {}".format(accuracy_score(ytest,y_pred3)))
print("f1_Score {}".format(f1_score(y_pred3,ytest,average='weighted')))

model = RandomForestClassifier(verbose=2,n_estimators=200,max_features='log2',max_depth=None,criterion='entropy')
model.fit(xtrain,ytrain)

import pickle
pickle.dump(model,open('liver_analysis.pk1','wb'))

pickle.dump(scaler,open('scaling.pkl','wb'))

"""**TESTING WITH VALUES**"""

input=[[65,1,0.7,0.1,187,16,18,6.8,3.3,0.90]]
input=scaler.transform(input)
prediction = model.predict(input)
prediction

input=[[17,0,0.9,0.3,202,22,19,7.4,4.1,1.2]]
input=scaler.transform(input)
prediction = model.predict(input)
prediction

input=[[33,0,3.4,1.6,186,779,884,7.3,3.2,0.7]]
input=scaler.transform(input)
prediction = model.predict(input)
prediction



input=[[26,0,0.6,0.2,120,45,51,7.9,.4,1]]
input=scaler.transform(input)
prediction = RFmodel.predict(input)
prediction
